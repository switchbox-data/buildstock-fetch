{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"buildstock-fetch","text":"<p>A CLI tool to simplify downloading building characteristics and load curve data from NREL's ResStock and ComStock projects. Also available as a Python library.</p>"},{"location":"#installing-bsf","title":"Installing bsf","text":"<p>To install the buildstock-fetch CLI tool, we recommend using uv or pipx:</p> <pre><code>uv tool install buildstock-fetch\n</code></pre> <p>or</p> <pre><code>pipx install buildstock-fetch\n</code></pre> <p>You'll then be able to access the <code>bsf</code> command system-wide:</p> <pre><code>bsf --help\n</code></pre>"},{"location":"#using-bsf","title":"Using bsf","text":"<p>To make it easy to download what you want from NREL's S3 bucket, <code>bsf</code> has an interactive mode. Activate it with:</p> <pre><code>bsf\n</code></pre> <p>Alternatively, you can tell <code>bsf</code> exactly what to download via CLI args:</p> <pre><code>bsf --product resstock --release_year 2022 --weather_file tmy3 --release_version 1 --states CA --file_type \"hpxml metadata\" --upgrade_id \"0 1 2\" --output_directory ./CA_data\n</code></pre> <p>For more details about the usage, see Usage</p>"},{"location":"#installing-the-python-library","title":"Installing the Python library","text":"<p>buildstock-fetch is implemented in Python, and we expose our internal functions via a library.</p> <p>If you're using Python, and you want to install the CLI only for a particular project (rather than system-wide), or you want to user the underlying library, install our PyPI package via:</p> <pre><code>pip install buildstock-fetch\n</code></pre> <p>or</p> <pre><code>uv add buildstock-fetch\n</code></pre>"},{"location":"mixed_upgrades/","title":"Mixed Upgrade Scenarios","text":"<p>This guide demonstrates how to use <code>BuildStockRead</code> and <code>MixedUpgradeScenario</code> to model building upgrade adoption over time.</p>"},{"location":"mixed_upgrades/#overview","title":"Overview","text":"<p>The mixed upgrade scenario functionality allows you to:</p> <ul> <li>Model heterogeneous upgrade mixes where buildings adopt different upgrades</li> <li>Simulate multi-year adoption trajectories</li> <li>Ensure monotonic adoption (buildings can only move from baseline to upgraded, never backwards)</li> <li>Read and analyze data across different upgrade scenarios and time periods</li> </ul>"},{"location":"mixed_upgrades/#quick-start","title":"Quick Start","text":""},{"location":"mixed_upgrades/#1-define-a-scenario","title":"1. Define a Scenario","text":"<p>Use the <code>uniform_adoption</code> helper to create a scenario where buildings progressively adopt upgrades over time:</p> <pre><code>from buildstock_fetch.scenarios import uniform_adoption\n\n# Define a scenario with 2 upgrades over 3 years\n# 60% of adopters choose upgrade 4, 40% choose upgrade 8\n# Total adoption grows from 10% to 30% to 50% over 3 years\nscenario = uniform_adoption(\n    upgrade_ids=[4, 8],\n    weights={4: 0.6, 8: 0.4},\n    adoption_trajectory=[0.1, 0.3, 0.5],\n)\n\n# This creates:\n# {4: [0.06, 0.18, 0.30], 8: [0.04, 0.12, 0.20]}\n# Year 0: 6% upgrade 4, 4% upgrade 8, 90% baseline\n# Year 1: 18% upgrade 4, 12% upgrade 8, 70% baseline\n# Year 2: 30% upgrade 4, 20% upgrade 8, 50% baseline\n</code></pre>"},{"location":"mixed_upgrades/#2-create-a-mixed-upgrade-scenario","title":"2. Create a Mixed Upgrade Scenario","text":"<pre><code>from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario\n\nmus = MixedUpgradeScenario(\n    data_path=\"./data\",                  # Path to downloaded data\n    release=\"res_2024_tmy3_2\",           # BuildStock release\n    states=\"NY\",                         # State(s) to analyze\n    sample_n=1000,                       # Sample 1000 buildings (optional)\n    random=42,                           # Random seed for reproducibility\n    scenario=scenario,                   # Adoption scenario\n)\n</code></pre>"},{"location":"mixed_upgrades/#3-read-and-analyze-data","title":"3. Read and Analyze Data","text":"<pre><code># Read metadata for all years\nmetadata = mus.read_metadata().collect()\nprint(metadata.head())\n\n# Read metadata for specific years only\nmetadata_years_0_1 = mus.read_metadata(years=[0, 1]).collect()\n\n# Read 15-minute load curves\nload_curves = mus.read_load_curve_15min().collect()\n\n# Export scenario to CAIRO-compatible CSV format\nmus.export_scenario_to_cairo(\"./scenario.csv\")\n</code></pre>"},{"location":"mixed_upgrades/#reading-buildstock-data-with-buildstockread","title":"Reading BuildStock Data with BuildStockRead","text":"<p>Before using <code>MixedUpgradeScenario</code>, you can use <code>BuildStockRead</code> to explore individual releases:</p> <pre><code>from buildstock_fetch.read import BuildStockRead\n\n# Initialize reader for a specific release and state\nbsr = BuildStockRead(\n    data_path=\"./data\",\n    release=\"res_2024_tmy3_2\",\n    states=\"NY\",\n    sample_n=500,\n    random=42,\n)\n\n# Read baseline metadata (upgrade 0)\nbaseline = bsr.read_metadata(upgrades=\"0\").collect()\n\n# Read specific upgrade\nupgrade_4 = bsr.read_metadata(upgrades=\"4\").collect()\n\n# Read load curves for a specific upgrade\nload_curves = bsr.read_load_curve_15min(upgrades=\"4\").collect()\n\n# Filter by building IDs\nspecific_buildings = bsr.read_metadata(\n    upgrades=\"0\",\n    building_ids=[123456, 234567, 345678]\n).collect()\n</code></pre>"},{"location":"mixed_upgrades/#advanced-scenario-configuration","title":"Advanced Scenario Configuration","text":""},{"location":"mixed_upgrades/#custom-adoption-trajectories","title":"Custom Adoption Trajectories","text":"<p>You can manually define scenarios with custom adoption patterns:</p> <pre><code># Complex scenario with 3 upgrades\nscenario = {\n    4: [0.05, 0.15, 0.25, 0.35],   # Upgrade 4: grows from 5% to 35%\n    8: [0.03, 0.10, 0.18, 0.25],   # Upgrade 8: grows from 3% to 25%\n    12: [0.02, 0.05, 0.07, 0.10],  # Upgrade 12: grows from 2% to 10%\n}\n\nmus = MixedUpgradeScenario(\n    data_path=\"./data\",\n    release=\"res_2024_tmy3_2\",\n    states=[\"NY\", \"CA\"],            # Multiple states\n    scenario=scenario,\n)\n</code></pre>"},{"location":"mixed_upgrades/#multiple-states","title":"Multiple States","text":"<p>Analyze adoption across multiple states:</p> <pre><code>mus = MixedUpgradeScenario(\n    data_path=\"./data\",\n    release=\"res_2024_tmy3_2\",\n    states=[\"NY\", \"CA\", \"TX\"],      # Analyze 3 states\n    sample_n=2000,                   # Sample across all states\n    scenario=scenario,\n)\n\nmetadata = mus.read_metadata().collect()\n\n# Group by state to see state-specific adoption\nstate_summary = metadata.group_by([\"in.state\", \"year\", \"upgrade_id\"]).agg(\n    pl.count(\"bldg_id\").alias(\"count\")\n)\nprint(state_summary)\n</code></pre>"},{"location":"mixed_upgrades/#accessing-materialized-allocations","title":"Accessing Materialized Allocations","text":"<p>Get the exact building-to-upgrade mapping for each year:</p> <pre><code># Get materialized scenario (cached property)\nmaterialized = mus.materialized_scenario\n\n# materialized is a dict: {year_idx: {bldg_id: upgrade_id}}\n# Example:\n# {\n#   0: {405821: 0, 612547: 4, 789234: 0, ...},\n#   1: {405821: 0, 612547: 4, 789234: 8, ...},\n#   2: {405821: 4, 612547: 4, 789234: 8, ...}\n# }\n\n# Check which upgrade a specific building has in year 1\nbuilding_id = 612547\nyear_1_upgrade = materialized[1][building_id]\nprint(f\"Building {building_id} has upgrade {year_1_upgrade} in year 1\")\n</code></pre>"},{"location":"mixed_upgrades/#reading-different-data-types","title":"Reading Different Data Types","text":"<p><code>MixedUpgradeScenario</code> supports multiple temporal resolutions:</p> <pre><code># 15-minute resolution\nload_15min = mus.read_load_curve_15min(years=[0, 1, 2]).collect()\n\n# Hourly resolution\nload_hourly = mus.read_load_curve_hourly().collect()\n\n# Daily resolution\nload_daily = mus.read_load_curve_daily().collect()\n\n# Annual totals\nload_annual = mus.read_load_curve_annual().collect()\n\n# All methods return Polars LazyFrames - call .collect() to execute\n</code></pre>"},{"location":"mixed_upgrades/#exporting-for-cairo","title":"Exporting for CAIRO","text":"<p>Export your scenario to a CSV format compatible with CAIRO (or other analysis tools):</p> <pre><code>mus.export_scenario_to_cairo(\"./my_scenario.csv\")\n</code></pre> <p>Output format:</p> <pre><code>bldg_id,year_0,year_1,year_2\n405821,0,0,0\n612547,0,4,4\n789234,0,0,8\n</code></pre> <p>Each row is a building, each column is a year, and values are upgrade IDs.</p>"},{"location":"mixed_upgrades/#data-requirements","title":"Data Requirements","text":"<p>Before running mixed upgrade scenarios, ensure you have downloaded the required data:</p> <pre><code># Download metadata and load curves for all required upgrades\nbsf \\\n  --product resstock \\\n  --release_year 2024 \\\n  --weather_file tmy3 \\\n  --release_version 2 \\\n  --states \"NY CA\" \\\n  --file_type \"metadata load_curve_15min\" \\\n  --upgrade_id \"0 4 8\" \\\n  --output_directory ./data \\\n  --sample 100\n</code></pre> <p>If scenario data is missing, <code>MixedUpgradeScenario</code> will raise a <code>ScenarioDataNotFoundError</code> indicating which upgrades are missing.</p>"},{"location":"mixed_upgrades/#performance-tips","title":"Performance Tips","text":"<ol> <li>Sampling: Use <code>sample_n</code> to work with a representative subset of buildings</li> <li>Lazy Evaluation: <code>read_*</code> methods return LazyFrames - only call <code>.collect()</code> when you need results</li> <li>Specific Years: Read only needed years: <code>read_metadata(years=[0, 1])</code> instead of all years</li> <li>State Filtering: Download and analyze only relevant states</li> </ol>"},{"location":"mixed_upgrades/#example-complete-analysis-workflow","title":"Example: Complete Analysis Workflow","text":"<pre><code>from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario\nfrom buildstock_fetch.scenarios import uniform_adoption\nimport polars as pl\n\n# 1. Define adoption scenario\nscenario = uniform_adoption(\n    upgrade_ids=[4, 8],\n    weights={4: 0.7, 8: 0.3},\n    adoption_trajectory=[0.1, 0.2, 0.3, 0.4, 0.5],\n)\n\n# 2. Initialize scenario reader\nmus = MixedUpgradeScenario(\n    data_path=\"./data\",\n    release=\"res_2024_tmy3_2\",\n    states=[\"NY\", \"CA\"],\n    sample_n=5000,\n    random=42,\n    scenario=scenario,\n)\n\n# 3. Read and analyze metadata\nmetadata = mus.read_metadata().collect()\n\n# 4. Compute adoption statistics by year\nadoption_stats = metadata.group_by([\"year\", \"upgrade_id\"]).agg([\n    pl.count(\"bldg_id\").alias(\"building_count\"),\n    pl.col(\"bldg_id\").count().truediv(pl.len()).alias(\"fraction\"),\n])\n\nprint(\"Adoption over time:\")\nprint(adoption_stats.sort([\"year\", \"upgrade_id\"]))\n\n# 5. Analyze energy consumption by upgrade and year\nload_annual = mus.read_load_curve_annual().collect()\n\nenergy_by_upgrade = load_annual.group_by([\"year\", \"upgrade_id\"]).agg([\n    pl.col(\"out.electricity.total.energy_consumption\").sum().alias(\"total_electricity_kwh\"),\n    pl.col(\"out.natural_gas.total.energy_consumption\").sum().alias(\"total_gas_therm\"),\n])\n\nprint(\"\\nEnergy consumption by upgrade scenario:\")\nprint(energy_by_upgrade.sort([\"year\", \"upgrade_id\"]))\n\n# 6. Export scenario for CAIRO\nmus.export_scenario_to_cairo(\"./scenario.csv\")\n\nprint(\"\\nScenario exported to scenario.csv\")\n</code></pre>"},{"location":"mixed_upgrades/#see-also","title":"See Also","text":"<ul> <li>BuildStock Fetch Usage Guide - General usage and data downloading</li> <li>API Reference - Complete API documentation</li> <li>NREL End-Use Load Profiles - Data source information</li> </ul>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#buildstock_fetch.main.fetch_bldg_ids","title":"<code>buildstock_fetch.main.fetch_bldg_ids(product, release_year, weather_file, release_version, state, upgrade_id)</code>","text":"<p>Fetch a list of Building ID's</p> <p>Provided a state, returns a list of building ID's for that state.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>ResCom</code> <p>The product type (e.g., 'resstock', 'comstock')</p> required <code>release_year</code> <code>ReleaseYear</code> <p>The release year (e.g., '2021', '2022')</p> required <code>weather_file</code> <code>Weather</code> <p>The weather file type (e.g., 'tmy3')</p> required <code>release_version</code> <code>str</code> <p>The release version number (e.g., '1')</p> required <code>state</code> <code>str</code> <p>The state to fetch building ID's for.</p> required <p>Returns:</p> Type Description <code>list[BuildingID]</code> <p>A list of building ID's for the given state.</p> Source code in <code>buildstock_fetch/main.py</code> <pre><code>def fetch_bldg_ids(\n    product: ResCom, release_year: ReleaseYear, weather_file: Weather, release_version: str, state: str, upgrade_id: str\n) -&gt; list[BuildingID]:\n    \"\"\"Fetch a list of Building ID's\n\n    Provided a state, returns a list of building ID's for that state.\n\n    Args:\n        product: The product type (e.g., 'resstock', 'comstock')\n        release_year: The release year (e.g., '2021', '2022')\n        weather_file: The weather file type (e.g., 'tmy3')\n        release_version: The release version number (e.g., '1')\n        state: The state to fetch building ID's for.\n\n    Returns:\n        A list of building ID's for the given state.\n    \"\"\"\n\n    if product == \"resstock\":\n        product_str = \"res\"\n    elif product == \"comstock\":\n        product_str = \"com\"\n    else:\n        raise InvalidProductError(product)\n\n    release_name = f\"{product_str}_{release_year}_{weather_file}_{release_version}\"\n    if not _validate_release_name(release_name):\n        raise InvalidReleaseNameError(release_name)\n\n    # Read the specific partition that matches our criteria\n    partition_path = (\n        METADATA_DIR\n        / f\"product={product}\"\n        / f\"release_year={release_year}\"\n        / f\"weather_file={weather_file}\"\n        / f\"release_version={release_version}\"\n        / f\"state={state}\"\n    )\n\n    # Check if the partition exists\n    if not partition_path.exists():\n        return []\n\n    # Read the parquet files in the specific partition\n    df = pl.read_parquet(str(partition_path))\n\n    # No need to filter since we're already reading the specific partition\n    filtered_df = df\n\n    # Convert the filtered data to BuildingID objects\n    building_ids = []\n    for row in filtered_df.iter_rows(named=True):\n        building_id = BuildingID(\n            bldg_id=int(row[\"bldg_id\"]),\n            release_number=release_version,\n            release_year=release_year,\n            res_com=product,\n            weather=weather_file,\n            upgrade_id=upgrade_id,\n            state=state,\n        )\n        building_ids.append(building_id)\n\n    return building_ids\n</code></pre>"},{"location":"modules/#buildstock_fetch.main.fetch_bldg_data","title":"<code>buildstock_fetch.main.fetch_bldg_data(bldg_ids, file_type, output_dir, max_workers=5, weather_states=None)</code>","text":"<p>Download building data for a given list of building ids</p> <p>Downloads the data for the given building ids and returns list of paths to the downloaded files.</p> <p>Parameters:</p> Name Type Description Default <code>bldg_ids</code> <code>list[BuildingID]</code> <p>A list of BuildingID objects to download data for.</p> required <p>Returns:</p> Type Description <code>tuple[list[Path], list[str]]</code> <p>A list of paths to the downloaded files.</p> Source code in <code>buildstock_fetch/main.py</code> <pre><code>def fetch_bldg_data(\n    bldg_ids: list[BuildingID],\n    file_type: tuple[str, ...],\n    output_dir: Path,\n    max_workers: int = 5,\n    weather_states: list[str] | None = None,\n) -&gt; tuple[list[Path], list[str]]:\n    \"\"\"Download building data for a given list of building ids\n\n    Downloads the data for the given building ids and returns list of paths to the downloaded files.\n\n    Args:\n        bldg_ids: A list of BuildingID objects to download data for.\n\n    Returns:\n        A list of paths to the downloaded files.\n    \"\"\"\n    file_type_obj = _parse_requested_file_type(file_type)\n    console = Console()\n\n    # Initialize weather_states to empty list if None\n    if weather_states is None:\n        weather_states = []\n\n    downloaded_paths: list[Path] = []\n    failed_downloads: list[str] = []\n\n    # Calculate total files to download\n    total_files = 0\n    if file_type_obj.metadata:\n        unique_metadata_urls = _resolve_unique_metadata_urls(bldg_ids)\n        total_files += len(unique_metadata_urls)  # Add metadata file\n    if file_type_obj.load_curve_15min:\n        total_files += len(bldg_ids)  # Add 15-minute load curve files\n    if file_type_obj.load_curve_hourly:\n        total_files += len(bldg_ids)  # Add hourly load curve files\n    if file_type_obj.load_curve_daily:\n        total_files += len(bldg_ids)  # Add daily load curve files\n    if file_type_obj.load_curve_monthly:\n        total_files += len(bldg_ids)  # Add monthly load curve files\n    if file_type_obj.load_curve_annual:\n        total_files += len(bldg_ids)  # Add annual load curve files\n    if file_type_obj.weather:\n        available_bldg_ids = [bldg_id for bldg_id in bldg_ids if bldg_id.state in weather_states]\n        total_files += len(available_bldg_ids) * len(weather_states)  # Add weather map files\n\n    console.print(f\"\\n[bold blue]Starting download of {total_files} files...[/bold blue]\")\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TextColumn(\"\u2022\"),\n        DownloadColumn(),\n        TextColumn(\"\u2022\"),\n        TransferSpeedColumn(),\n        TextColumn(\"\u2022\"),\n        TimeRemainingColumn(),\n        console=console,\n        transient=False,\n    ) as progress:\n        _execute_downloads(\n            file_type_obj,\n            bldg_ids,\n            output_dir,\n            max_workers,\n            progress,\n            downloaded_paths,\n            failed_downloads,\n            console,\n            weather_states,\n        )\n\n        # TODO: add EV related files\n        # TODO: Write a function for downloading EV related files from SB's s3 bucket.\n        # It should dynamically build the download url based on the release_name + state combo.\n        # Make sure to follow the directory structure for downloading the files.\n        if file_type_obj.trip_schedules:\n            _download_trip_schedules_data(bldg_ids, output_dir, downloaded_paths)\n\n    _print_download_summary(downloaded_paths, failed_downloads, console)\n\n    return downloaded_paths, failed_downloads\n</code></pre>"},{"location":"modules/#buildstock_fetch.read.BuildStockRead","title":"<code>buildstock_fetch.read.BuildStockRead</code>","text":"<p>Reader class for BuildStock data downloaded with bsf.</p> <p>This class provides methods to read metadata and load curve data from locally downloaded BuildStock files.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path | S3Path | str</code> <p>Path to the data directory (local path or S3 path).</p> required <code>release</code> <code>ReleaseKey | BuildstockRelease</code> <p>A BuildStockRelease enum member specifying the release.</p> required <code>states</code> <code>USStateCode | Collection[USStateCode] | None</code> <p>Optional State or list of States to filter data. If None, auto-detects states present on disk.</p> <code>None</code> <code>sample_n</code> <code>int | None</code> <p>Optional number of buildings to sample.</p> <code>None</code> <code>random</code> <code>Random | int | None</code> <p>Optional random state for reproducible sampling (Random instance or int seed).</p> <code>None</code> <code>metadata_variant</code> <code>Literal['standard', 'sb']</code> <p>Metadata file variant to use. \"standard\" for metadata.parquet, \"sb\" for metadata-sb.parquet (Switchbox-specific). Defaults to \"standard\".</p> <code>'standard'</code> Example <p>from buildstock_fetch.read import BuildStockRead bsr = BuildStockRead( ...     data_path=\"./data\", ...     release=\"res_2024_tmy_2\", ...     states=\"NY\", ... ) metadata = bsr.read_metadata(upgrades=[\"0\", \"1\"])</p> Source code in <code>buildstock_fetch/read.py</code> <pre><code>@final\nclass BuildStockRead:\n    \"\"\"Reader class for BuildStock data downloaded with bsf.\n\n    This class provides methods to read metadata and load curve data\n    from locally downloaded BuildStock files.\n\n    Args:\n        data_path: Path to the data directory (local path or S3 path).\n        release: A BuildStockRelease enum member specifying the release.\n        states: Optional State or list of States to filter data.\n            If None, auto-detects states present on disk.\n        sample_n: Optional number of buildings to sample.\n        random: Optional random state for reproducible sampling (Random instance or int seed).\n        metadata_variant: Metadata file variant to use. \"standard\" for metadata.parquet,\n            \"sb\" for metadata-sb.parquet (Switchbox-specific). Defaults to \"standard\".\n\n    Example:\n        &gt;&gt;&gt; from buildstock_fetch.read import BuildStockRead\n        &gt;&gt;&gt; bsr = BuildStockRead(\n        ...     data_path=\"./data\",\n        ...     release=\"res_2024_tmy_2\",\n        ...     states=\"NY\",\n        ... )\n        &gt;&gt;&gt; metadata = bsr.read_metadata(upgrades=[\"0\", \"1\"])\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: Path | S3Path | str,\n        release: ReleaseKey | BuildstockRelease,\n        states: USStateCode | Collection[USStateCode] | None = None,\n        sample_n: int | None = None,\n        random: Random | int | None = None,\n        metadata_variant: Literal[\"standard\", \"sb\"] = \"standard\",\n    ) -&gt; None:\n        self.data_path = S3Path(cast(str, data_path)) if is_s3_path(data_path) else Path(cast(str, data_path))\n        self.release = release if isinstance(release, BuildstockRelease) else BuildstockReleases.load()[release]\n\n        self.states: list[USStateCode] | None\n        if states is None:\n            self.states = None\n        elif is_valid_state_code(states):\n            self.states = [states]\n        else:\n            self.states = list(states)\n\n        if random is None:\n            self.random = Random()\n        elif isinstance(random, Random):\n            self.random = random\n        else:\n            self.random = Random(random)\n\n        self.sample_n = sample_n\n        self.metadata_variant = metadata_variant\n\n    @cached_property\n    def downloaded_metadata(self) -&gt; DownloadedData:\n        return DownloadedData(\n            filter_downloads(\n                self.data_path,\n                release_key=(self.release.key,),\n                state=self.states,\n                file_type=\"metadata\",\n            )\n        )\n\n    @cached_property\n    def sampled_buildings(self) -&gt; frozenset[int] | None:\n        if self.sample_n is None:\n            return None\n        metadata_files = self.downloaded_metadata.filter(file_type=\"metadata\", suffix=\".parquet\")\n        if not metadata_files:\n            raise MetadataNotFoundError(self.release, self.states)\n\n        # Select only the chosen metadata variant\n        metadata_filename = \"metadata-sb.parquet\" if self.metadata_variant == \"sb\" else \"metadata.parquet\"\n        metadata_files = DownloadedData(f for f in metadata_files if f.filename == metadata_filename)\n        if not metadata_files:\n            raise MetadataNotFoundError(self.release, self.states)\n\n        # Get unique states and find minimum upgrade per state\n        # This ensures sampling from all states when multiple states are requested\n        states = {f.state for f in metadata_files}\n        files_to_read = []\n        for state in states:\n            state_files = [f for f in metadata_files if f.state == state]\n            min_upgrade = min(state_files, key=lambda f: int(f.upgrade)).upgrade\n            files_to_read.extend([f for f in state_files if f.upgrade == min_upgrade])\n\n        df = pl.scan_parquet([str(f.file_path) for f in files_to_read]).select(\"bldg_id\").collect()\n        all_building_ids = cast(list[int], df[\"bldg_id\"].unique().to_list())\n\n        if self.sample_n &gt; len(all_building_ids):\n            logging.getLogger(__name__).info(\n                f\"sample_n ({self.sample_n}) exceeds available buildings ({len(all_building_ids)}). \"\n                + \"Returning all buildings without sampling.\"\n            )\n            return frozenset(all_building_ids)\n\n        return frozenset(self.random.sample(all_building_ids, self.sample_n))\n\n    def read_metadata(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"metadata\", upgrades, building_ids)\n\n    def read_load_curve_15min(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"load_curve_15min\", upgrades, building_ids)\n\n    def read_load_curve_hourly(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"load_curve_hourly\", upgrades, building_ids)\n\n    def read_load_curve_daily(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"load_curve_daily\", upgrades, building_ids)\n\n    def read_load_curve_monthly(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"load_curve_monthly\", upgrades, building_ids)\n\n    def read_load_curve_annual(\n        self, upgrades: str | Collection[str] | None = None, building_ids: Collection[int] | None = None\n    ) -&gt; pl.LazyFrame:\n        return self.read_parquets(\"load_curve_annual\", upgrades, building_ids)\n\n    def read_parquets(\n        self,\n        file_type: FileType,\n        upgrades: str | Collection[str] | None = None,\n        building_ids: Collection[int] | None = None,\n    ) -&gt; pl.LazyFrame:\n        if \"metadata\" not in self.release.file_types:\n            raise FileTypeNotAvailableError(self.release, file_type)\n\n        upgrades = self._validate_upgrades(file_type, upgrades)\n\n        # We use different reading strategies based on file type:\n        # - Metadata files: Must use diagonal concat because upgrade 0 metadata files have missing\n        #   upgrade columns compared to other upgrades, creating schema mismatches which would cause\n        #   scan_parquet with globbed files paths to fail. Diagonal concat handles this by filling\n        #   missing columns with nulls.\n        # - Load curve files: Can use a single glob pattern with hive partitioning since schemas\n        #   are consistent across upgrades.\n\n        if file_type == \"metadata\":\n            lf = self._read_metadata_with_diagonal_concat(file_type)\n        else:\n            lf = pl.scan_parquet(str(self.data_path / self.release.key / file_type) + \"/\")\n\n        # Apply all filters using Polars (states, upgrades, building_ids, sampled_buildings)\n        # Polars automatically infers state/upgrade columns from hive partitioning\n        lf = self._apply_filters(lf, upgrades, building_ids)\n\n        return lf\n\n    def _read_metadata_with_diagonal_concat(self, file_type: FileType) -&gt; pl.LazyFrame:\n        \"\"\"Read all metadata files and concat diagonally, then filters will be applied.\"\"\"\n        # Get all metadata files (not filtered by state/upgrade - we'll filter with Polars)\n        files = self.downloaded_metadata.filter(file_type=file_type, suffix=\".parquet\")\n\n        # For metadata files, filter to chosen variant\n        if file_type == \"metadata\":\n            metadata_filename = \"metadata-sb.parquet\" if self.metadata_variant == \"sb\" else \"metadata.parquet\"\n            files = DownloadedData(f for f in files if f.filename == metadata_filename)\n\n        file_paths = [str(file.file_path) for file in files]\n\n        if not file_paths:\n            return pl.LazyFrame()\n\n        # Scan each file with hive partitioning and concat diagonally\n        # This handles schema mismatches where different upgrades have different columns\n        # (e.g., upgrade 0 metadata files have missing upgrade columns)\n        lazy_frames = [pl.scan_parquet(file_path, hive_partitioning=True) for file_path in file_paths]\n        # Type checker doesn't know concat returns LazyFrame when given LazyFrames\n        result = pl.concat(lazy_frames, how=\"diagonal\")\n        return cast(pl.LazyFrame, result)\n\n    def _apply_filters(\n        self,\n        lf: pl.LazyFrame,\n        upgrades: frozenset[UpgradeID] | None = None,\n        building_ids: Collection[int] | None = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"Apply all filters (states, upgrades, building_ids, sampled_buildings) using Polars.\n\n        Filter order: Partition filters (state/upgrade) are applied first as they can prune\n        entire files/partitions via hive partitioning. Row-level filters (bldg_id) are applied\n        after and use predicate pushdown with row group statistics. Polars query optimizer\n        collects all predicates automatically, but ordering partition filters first helps\n        ensure they're recognized for file pruning.\n        \"\"\"\n\n        # Apply state and upgradepartition filters first - these can prune entire files/partitions\n        # See: https://pola.rs/posts/predicate-pushdown-query-optimizer/\n\n        # Apply state filter if states are specified\n        if self.states is not None:\n            lf = lf.filter(pl.col(\"state\").is_in(self.states))\n\n        # Apply upgrade filter if upgrades are specified\n        # Hive partitions treats upgrades as int64\n        if upgrades:\n            upgrade_values = [int(upgrade) for upgrade in upgrades]\n            lf = lf.filter(pl.col(\"upgrade\").is_in(upgrade_values))\n\n        # Apply row-level filters - combine multiple bldg_id filters for efficiency\n        building_id_sets = []\n        if building_ids is not None:\n            building_id_sets.append(set(building_ids))\n        if self.sampled_buildings is not None:\n            building_id_sets.append(set(self.sampled_buildings))\n\n        if building_id_sets:\n            # Intersect all building ID sets and apply single filter\n            combined_ids = set.intersection(*building_id_sets) if len(building_id_sets) &gt; 1 else building_id_sets[0]\n            if combined_ids:\n                lf = lf.filter(pl.col(\"bldg_id\").is_in(list(combined_ids)))\n\n        return lf\n\n    def _available_upgrades(self, file_type: FileType) -&gt; frozenset[UpgradeID]:\n        if file_type == \"metadata\":\n            return self.downloaded_metadata.filter(state=self.states, file_type=file_type).upgrades()\n\n        state_upgrade_ids = []\n        target_path = self.data_path / self.release.key / file_type\n        if not target_path.exists():\n            raise NoUpgradesFoundError(self.release)\n        for state_path in target_path.iterdir():\n            # Note: This is a little weird if multiple states. Currently returns intersection,\n            # so only upgrades available in all states are returned.\n            if self.states is None or state_path.name.removeprefix(\"state=\") in self.states:\n                state_upgrade_ids.append({u.name.removeprefix(\"upgrade=\") for u in state_path.iterdir()})\n        return frozenset(normalize_upgrade_id(_) for _ in set.intersection(*state_upgrade_ids))\n\n    def _validate_upgrades(\n        self, file_type: FileType, upgrades: str | Collection[str] | None = None\n    ) -&gt; frozenset[UpgradeID]:\n        if upgrades is None:\n            upgrades = None\n        elif isinstance(upgrades, str):\n            upgrades = [normalize_upgrade_id(upgrades)]\n        else:\n            upgrades = [normalize_upgrade_id(_) for _ in upgrades]\n\n        # We shouldn't raise an error here - an empty list may be passed intentionally\n        if upgrades is not None and not upgrades:\n            logging.getLogger(__name__).info(\"Empty upgrades list got passed into validate_upgrades\")\n            return frozenset()\n\n        if upgrades and (invalid_upgrades := [_ for _ in upgrades if _ not in self.release.upgrades]):\n            raise InvalidUpgradeForRelease(self.release, *cast(tuple[UpgradeID, ...], tuple(invalid_upgrades)))\n\n        available_upgrades = self._available_upgrades(file_type)\n        if not available_upgrades:\n            raise NoUpgradesFoundError(self.release)\n\n        if upgrades and (missing_upgrades := [_ for _ in upgrades if _ not in available_upgrades]):\n            raise UpgradeNotFoundError(\n                self.release, available_upgrades, *cast(tuple[UpgradeID, ...], tuple(missing_upgrades))\n            )\n\n        if upgrades:\n            return frozenset(cast(Collection[UpgradeID], upgrades))\n        return frozenset(available_upgrades)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario","title":"<code>buildstock_fetch.mixed_upgrade.MixedUpgradeScenario</code>","text":"<p>Class for orchestrating multi-year adoption trajectories across multiple upgrade scenarios.</p> <p>This class enables defining and reading heterogeneous upgrade mixes where buildings progressively adopt different upgrades over time. Buildings are sampled once from a baseline upgrade, then allocated to different upgrades according to adoption fractions per year. Monotonic adoption is enforced: buildings can only move from baseline to an upgrade, never backwards.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path | S3Path | str</code> <p>Path to the data directory (local path or S3 path).</p> required <code>scenario_name</code> <code>str</code> <p>Name of the pathway scenario. will be used to create subdirectories when writing out the metadata and load curves.</p> required <code>release</code> <code>ReleaseKey | BuildstockRelease</code> <p>A BuildstockRelease or release key string specifying the release.</p> required <code>states</code> <code>USStateCode | Collection[USStateCode] | None</code> <p>Optional state code or list of state codes to filter data. If None, auto-detects states present on disk.</p> <code>None</code> <code>sample_n</code> <code>int | None</code> <p>Optional number of buildings to sample from baseline.</p> <code>None</code> <code>random</code> <code>Random | int | None</code> <p>Optional Random instance or seed for reproducible sampling and allocation.</p> <code>None</code> <code>scenario</code> <code>dict[int, list[float]] | None</code> <p>Dict mapping upgrade IDs to adoption fractions per year. Example: {4: [0.06, 0.18, 0.30], 8: [0.04, 0.12, 0.20]} represents 3 years where upgrade 4 grows from 6% to 30% adoption and upgrade 8 grows from 4% to 20% adoption.</p> <code>None</code> Example <p>from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario from buildstock_fetch.scenarios import uniform_adoption scenario = uniform_adoption( ...     upgrade_ids=[4, 8], ...     weights={4: 0.6, 8: 0.4}, ...     adoption_trajectory=[0.1, 0.3, 0.5], ... ) mus = MixedUpgradeScenario( ...     data_path=\"./data\", ...     scenario_name=\"rapid_adoption\", ...     release=\"res_2024_tmy3_2\", ...     states=\"NY\", ...     sample_n=1000, ...     random=42, ...     scenario=scenario, ... ) metadata = mus.read_metadata().collect() mus.export_scenario_to_cairo(\"./scenario.csv\") mus.save_metadata_parquet() # writes to disk or S3 mus.save_hourly_load_parquet() # writes to disk or S3</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>@final\nclass MixedUpgradeScenario:\n    \"\"\"Class for orchestrating multi-year adoption trajectories across multiple upgrade scenarios.\n\n    This class enables defining and reading heterogeneous upgrade mixes where buildings\n    progressively adopt different upgrades over time. Buildings are sampled once from a\n    baseline upgrade, then allocated to different upgrades according to adoption fractions\n    per year. Monotonic adoption is enforced: buildings can only move from baseline to\n    an upgrade, never backwards.\n\n    Args:\n        data_path: Path to the data directory (local path or S3 path).\n        scenario_name: Name of the pathway scenario. will be used to create subdirectories when writing out the metadata and load curves.\n        release: A BuildstockRelease or release key string specifying the release.\n        states: Optional state code or list of state codes to filter data.\n            If None, auto-detects states present on disk.\n        sample_n: Optional number of buildings to sample from baseline.\n        random: Optional Random instance or seed for reproducible sampling and allocation.\n        scenario: Dict mapping upgrade IDs to adoption fractions per year.\n            Example: {4: [0.06, 0.18, 0.30], 8: [0.04, 0.12, 0.20]}\n            represents 3 years where upgrade 4 grows from 6% to 30% adoption\n            and upgrade 8 grows from 4% to 20% adoption.\n\n    Example:\n        &gt;&gt;&gt; from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario\n        &gt;&gt;&gt; from buildstock_fetch.scenarios import uniform_adoption\n        &gt;&gt;&gt; scenario = uniform_adoption(\n        ...     upgrade_ids=[4, 8],\n        ...     weights={4: 0.6, 8: 0.4},\n        ...     adoption_trajectory=[0.1, 0.3, 0.5],\n        ... )\n        &gt;&gt;&gt; mus = MixedUpgradeScenario(\n        ...     data_path=\"./data\",\n        ...     scenario_name=\"rapid_adoption\",\n        ...     release=\"res_2024_tmy3_2\",\n        ...     states=\"NY\",\n        ...     sample_n=1000,\n        ...     random=42,\n        ...     scenario=scenario,\n        ... )\n        &gt;&gt;&gt; metadata = mus.read_metadata().collect()\n        &gt;&gt;&gt; mus.export_scenario_to_cairo(\"./scenario.csv\")\n        &gt;&gt;&gt; mus.save_metadata_parquet() # writes to disk or S3\n        &gt;&gt;&gt; mus.save_hourly_load_parquet() # writes to disk or S3\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: Path | S3Path | str,\n        scenario_name: str,\n        release: ReleaseKey | BuildstockRelease,\n        states: USStateCode | Collection[USStateCode] | None = None,\n        sample_n: int | None = None,\n        random: Random | int | None = None,\n        scenario: dict[int, list[float]] | None = None,\n    ) -&gt; None:\n        if scenario is None:\n            raise ScenarioParameterRequiredError()\n\n        validate_scenario(scenario)\n        self.scenario = scenario\n        self.scenario_name = scenario_name\n        self._upgrade_ids = tuple(scenario)\n        self.num_years = len(next(iter(scenario.values())))\n\n        self.release = release if isinstance(release, BuildstockRelease) else BuildstockReleases.load()[release]\n\n        if states is None:\n            self.states: list[USStateCode] | None = None\n        elif is_valid_state_code(states):\n            self.states = [states]\n        else:\n            self.states = list(states)\n\n        self.random = random if isinstance(random, Random) else Random(random)\n\n        self.sample_n = sample_n\n        self._available_bldg_ids_cache: dict[str, dict[int, frozenset[int]]] = {}\n\n        # Normalize the path first so BuildStockRead recognizes S3 URLs in Path objects\n        normalized_data_path = _resolve_path(data_path)\n\n        # Baseline reader handles state detection and optional sampling.\n        self.baseline_reader = BuildStockRead(\n            data_path=normalized_data_path,\n            release=self.release,\n            states=self.states,\n            sample_n=sample_n,\n            random=self.random,\n            metadata_variant=\"standard\",  # Always use standard variant for mixed upgrades\n        )\n        # Reuse the normalized path from BuildStockRead (supports both local and S3)\n        self.data_path = self.baseline_reader.data_path\n\n        sampled = self.baseline_reader.sampled_buildings\n        if sampled is not None:\n            self.sampled_bldgs: frozenset[int] = sampled\n        else:\n            # No sampling: fall back to full baseline bldg_id list.\n            metadata_files = self.downloaded_metadata.filter(suffix=\".parquet\", upgrade=normalize_upgrade_id(\"0\"))\n            if not metadata_files:\n                raise BaselineMetadataNotFoundError()\n            first_file = min(metadata_files, key=lambda x: x.file_path)\n            df = pl.scan_parquet(str(first_file.file_path)).select(\"bldg_id\").collect()\n            all_bldg_ids = df[\"bldg_id\"].unique().to_list()\n            self.sampled_bldgs = frozenset(all_bldg_ids)\n\n        if not self.sampled_bldgs:\n            raise NoBuildingsAvailableError()\n\n        num_upgrades = len(scenario)\n        print(f\"Sampled {len(self.sampled_bldgs)} buildings from baseline (upgrade 0)\")\n        print(f\"Materialized {self.num_years} years of adoption across {num_upgrades} upgrades\")\n\n    @cached_property\n    def downloaded_metadata(self) -&gt; DownloadedData:\n        \"\"\"Cached property for metadata file discovery only.\"\"\"\n        return DownloadedData(\n            filter_downloads(\n                self.data_path,\n                release_key=(self.release.key,),\n                state=self.states,\n                file_type=\"metadata\",\n            )\n        )\n\n    @cached_property\n    def _allocation_plan(self) -&gt; tuple[list[int], dict[int, list[int]]]:\n        bldgs = list(self.sampled_bldgs)\n        self.random.shuffle(bldgs)\n        allocations: dict[int, list[int]] = {uid: [] for uid in self._upgrade_ids}\n        next_idx = 0\n        total = len(bldgs)\n\n        for year_idx in range(self.num_years):\n            for uid in self._upgrade_ids:\n                target = int(self.scenario[uid][year_idx] * total)\n                new = target - len(allocations[uid])\n                if new &gt; 0:\n                    # Assign contiguous slices once; yearly targets take prefixes.\n                    end = next_idx + new\n                    allocations[uid].extend(bldgs[next_idx:end])\n                    next_idx = end\n\n        return bldgs, allocations\n\n    @cached_property\n    def materialized_scenario(self) -&gt; dict[int, dict[int, int]]:\n        \"\"\"Materialize building allocations for all years in the scenario.\"\"\"\n        bldgs, allocations = self._allocation_plan\n        total = len(bldgs)\n        materialized: dict[int, dict[int, int]] = {}\n\n        for year_idx in range(self.num_years):\n            # Start from baseline, then overlay upgrades for this year.\n            year_map = dict.fromkeys(bldgs, 0)\n            for uid in self._upgrade_ids:\n                target = int(self.scenario[uid][year_idx] * total)\n                if target:\n                    for bldg_id in allocations[uid][:target]:\n                        year_map[bldg_id] = uid\n            materialized[year_idx] = year_map\n\n        return materialized\n\n    def _validate_years(self, years: list[int] | None) -&gt; list[int]:\n        \"\"\"Validate and resolve years parameter.\n\n        Args:\n            years: List of year indices, or None for all years.\n\n        Returns:\n            List of valid year indices.\n\n        Raises:\n            ValueError: If any year index is out of range.\n        \"\"\"\n        if years is None:\n            return list(range(self.num_years))\n\n        for year_idx in years:\n            if not 0 &lt;= year_idx &lt; self.num_years:\n                raise YearOutOfRangeError(year_idx, self.num_years - 1)\n\n        return years\n\n    def _scan_available_upgrades(self, file_type: FileType) -&gt; frozenset[UpgradeID]:\n        \"\"\"Scan parquet directories to discover available upgrades for a file type.\n\n        Uses directory structure scanning rather than file system traversal for performance.\n\n        Args:\n            file_type: Type of data to check (e.g., 'load_curve_hourly').\n\n        Returns:\n            Set of available upgrade IDs found in the directory structure.\n        \"\"\"\n        base_path = self.data_path / self.release.key / file_type\n\n        available_upgrades = set()\n        for state_dir in base_path.iterdir():\n            if self.states is None or state_dir.name.removeprefix(\"state=\") in self.states:\n                for upgrade_dir in state_dir.iterdir():\n                    upgrade_id = upgrade_dir.name.removeprefix(\"upgrade=\")\n                    available_upgrades.add(normalize_upgrade_id(upgrade_id))\n\n        return frozenset(available_upgrades)\n\n    def _validate_data_availability(self, file_type: FileType) -&gt; None:\n        \"\"\"Validate that all required upgrade data exists on disk.\n\n        Args:\n            file_type: Type of data to check (e.g., 'metadata', 'load_curve_15min').\n\n        Raises:\n            ScenarioDataNotFoundError: If data for any scenario upgrade is missing.\n        \"\"\"\n        required_upgrades = {normalize_upgrade_id(str(uid)) for uid in self._upgrade_ids} | {normalize_upgrade_id(\"0\")}\n\n        if file_type == \"metadata\":\n            # Use file system discovery for metadata\n            missing_upgrades = required_upgrades - self.downloaded_metadata.upgrades()\n        else:\n            # Use parquet directory scanning for load curves\n            available_upgrades = self._scan_available_upgrades(file_type)\n            missing_upgrades = required_upgrades - available_upgrades\n\n        if missing_upgrades:\n            raise ScenarioDataNotFoundError(file_type, missing_upgrades)\n\n    def _available_bldg_ids_by_upgrade(self, file_type: FileType) -&gt; dict[int, frozenset[int]]:\n        per_building_types = {\n            \"load_curve_15min\",\n            \"load_curve_hourly\",\n            \"load_curve_daily\",\n            \"load_curve_monthly\",\n            \"load_curve_annual\",\n        }\n        if file_type not in per_building_types:\n            return {}\n\n        cached = self._available_bldg_ids_cache.get(file_type)\n        if cached is not None:\n            return cached\n\n        needed_upgrades = {0, *self._upgrade_ids}\n        available: dict[int, set[int]] = {uid: set() for uid in needed_upgrades}\n\n        # Scan parquet directory structure directly\n        base_path = self.data_path / self.release.key / file_type\n        for state_dir in base_path.iterdir():\n            if self.states is None or state_dir.name.removeprefix(\"state=\") in self.states:\n                for upgrade_dir in state_dir.iterdir():\n                    try:\n                        upgrade_id = int(upgrade_dir.name.removeprefix(\"upgrade=\"))\n                        if upgrade_id not in available:\n                            continue\n\n                        # List files and extract building IDs from filenames\n                        for file_path in upgrade_dir.glob(\"*.parquet\"):\n                            match = re.match(r\"^(\\d+)-\", file_path.name)\n                            if match:\n                                bldg_id = int(match.group(1))\n                                available[upgrade_id].add(bldg_id)\n                    except (ValueError, AttributeError):\n                        continue\n\n        cached = {uid: frozenset(ids) for uid, ids in available.items()}\n        self._available_bldg_ids_cache[file_type] = cached\n        return cached\n\n    def _warn_on_missing_ids(\n        self,\n        file_type: FileType,\n        upgrade_id: int,\n        year_idx: int,\n        requested_ids: list[int],\n        available_ids: frozenset[int],\n        sample_size: int = 10,\n    ) -&gt; None:\n        if not requested_ids or not available_ids:\n            return\n        missing = [bldg_id for bldg_id in requested_ids if bldg_id not in available_ids]\n        if not missing:\n            return\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            \"Missing %s files for upgrade %s in year_idx %s: %s of %s requested IDs not found%s\",\n            file_type,\n            upgrade_id,\n            year_idx,\n            len(missing),\n            len(requested_ids),\n            f\" (example IDs: {missing[:sample_size]})\" if sample_size else \"\",\n        )\n\n    def _read_for_upgrade(\n        self,\n        file_type: FileType,\n        upgrade_id: int,\n        bldg_ids: Iterable[int],\n        year_idx: int,\n        available_ids_by_upgrade: dict[int, frozenset[int]],\n    ) -&gt; pl.LazyFrame | None:\n        \"\"\"Read data for a specific upgrade and set of building IDs.\"\"\"\n        bldg_ids_list = list(bldg_ids)\n        if not bldg_ids_list:\n            return None\n        if available_ids_by_upgrade:\n            available_ids = available_ids_by_upgrade.get(upgrade_id, frozenset())\n            self._warn_on_missing_ids(file_type, upgrade_id, year_idx, bldg_ids_list, available_ids)\n        lf = self.baseline_reader.read_parquets(file_type, upgrades=str(upgrade_id), building_ids=bldg_ids_list)\n        if file_type == \"metadata\":\n            lf = lf.rename({\"upgrade\": \"upgrade_id\"})\n        else:\n            lf = lf.with_columns(pl.lit(upgrade_id).alias(\"upgrade_id\"))\n        return lf.with_columns(pl.lit(year_idx).alias(\"year\"))\n\n    def _process_year(\n        self,\n        file_type: FileType,\n        year_idx: int,\n        bldgs: list[int],\n        allocations: dict[int, list[int]],\n        total: int,\n        available_ids_by_upgrade: dict[int, frozenset[int]],\n    ) -&gt; pl.LazyFrame | None:\n        \"\"\"Process data for a single year, reading all upgrades and baseline.\"\"\"\n        year_dfs: list[pl.LazyFrame] = []\n        total_adopted = 0\n\n        for uid in self._upgrade_ids:\n            target = int(self.scenario[uid][year_idx] * total)\n            total_adopted += target\n            lf = self._read_for_upgrade(file_type, uid, allocations[uid][:target], year_idx, available_ids_by_upgrade)\n            if lf is not None:\n                year_dfs.append(lf)\n\n        # Baseline = remaining buildings after all upgrade allocations.\n        lf = self._read_for_upgrade(file_type, 0, bldgs[total_adopted:], year_idx, available_ids_by_upgrade)\n        if lf is not None:\n            year_dfs.append(lf)\n\n        if year_dfs:\n            return pl.concat(year_dfs, how=\"vertical_relaxed\")\n        return None\n\n    def _read_data_for_scenario(self, file_type: FileType, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read data across upgrades and years without full materialization.\"\"\"\n        years = sorted(self._validate_years(years))\n        self._validate_data_availability(file_type)\n        available_ids_by_upgrade = self._available_bldg_ids_by_upgrade(file_type)\n        # Build order + per-upgrade adopter lists once; slice per year.\n        bldgs, allocations = self._allocation_plan\n        total = len(bldgs)\n\n        all_year_dfs: list[pl.LazyFrame] = []\n        for year_idx in years:\n            year_df = self._process_year(file_type, year_idx, bldgs, allocations, total, available_ids_by_upgrade)\n            if year_df is not None:\n                all_year_dfs.append(year_df)\n\n        if not all_year_dfs:\n            return pl.LazyFrame({\"bldg_id\": [], \"upgrade_id\": [], \"year\": []})\n\n        return pl.concat(all_year_dfs, how=\"vertical_relaxed\")\n\n    def read_metadata(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read metadata for specified years in the scenario.\n\n        Returns a LazyFrame containing metadata for all buildings and years in the\n        scenario. Each row represents one building in one year.\n\n        Args:\n            years: List of year indices to include (0-indexed), or None for all years.\n                Example: [0, 1, 2] or None\n\n        Returns:\n            A Polars LazyFrame with columns:\n                - bldg_id: Building ID (from sampled baseline)\n                - upgrade_id: Upgrade ID for this building in this year (0 or scenario upgrade)\n                - year: Year index (0-indexed)\n                - ...: Original metadata columns (e.g., in.state, in.vintage, etc.)\n\n        Raises:\n            ValueError: If any year index is out of range.\n            ScenarioDataNotFoundError: If metadata for scenario upgrades is not on disk.\n\n        Example:\n            &gt;&gt;&gt; # Read metadata for all years\n            &gt;&gt;&gt; metadata = mus.read_metadata()\n            &gt;&gt;&gt; df = metadata.collect()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Read metadata for specific years\n            &gt;&gt;&gt; metadata_early = mus.read_metadata(years=[0, 1])\n        \"\"\"\n        return self._read_data_for_scenario(\"metadata\", years)\n\n    def read_load_curve_15min(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read 15-minute load curve data for specified years in the scenario.\n\n        Args:\n            years: List of year indices to include, or None for all years.\n\n        Returns:\n            A Polars LazyFrame with columns:\n                - bldg_id: Building ID\n                - upgrade_id: Upgrade ID\n                - year: Year index\n                - timestamp: Timestamp of the load data\n                - ...: Energy columns (e.g., out.electricity.total.energy_consumption)\n\n        Raises:\n            ValueError: If any year index is out of range.\n            ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n        \"\"\"\n        return self._read_data_for_scenario(\"load_curve_15min\", years)\n\n    def read_load_curve_hourly(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read hourly load curve data for specified years in the scenario.\n\n        Args:\n            years: List of year indices to include, or None for all years.\n\n        Returns:\n            A Polars LazyFrame with columns:\n                - bldg_id: Building ID\n                - upgrade_id: Upgrade ID\n                - year: Year index\n                - timestamp: Timestamp of the load data\n                - ...: Energy columns\n\n        Raises:\n            ValueError: If any year index is out of range.\n            ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n        \"\"\"\n        return self._read_data_for_scenario(\"load_curve_hourly\", years)\n\n    def read_load_curve_daily(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read daily load curve data for specified years in the scenario.\n\n        Args:\n            years: List of year indices to include, or None for all years.\n\n        Returns:\n            A Polars LazyFrame with columns:\n                - bldg_id: Building ID\n                - upgrade_id: Upgrade ID\n                - year: Year index\n                - timestamp: Timestamp of the load data\n                - ...: Energy columns\n\n        Raises:\n            ValueError: If any year index is out of range.\n            ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n        \"\"\"\n        return self._read_data_for_scenario(\"load_curve_daily\", years)\n\n    def read_load_curve_annual(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n        \"\"\"Read annual load curve data for specified years in the scenario.\n\n        Args:\n            years: List of year indices to include, or None for all years.\n\n        Returns:\n            A Polars LazyFrame with columns:\n                - bldg_id: Building ID\n                - upgrade_id: Upgrade ID\n                - year: Year index\n                - ...: Annual energy totals\n\n        Raises:\n            ValueError: If any year index is out of range.\n            ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n        \"\"\"\n        return self._read_data_for_scenario(\"load_curve_annual\", years)\n\n    def export_scenario_to_cairo(self, output_path: str | Path | S3Path) -&gt; None:\n        \"\"\"Export scenario to CAIRO-compatible CSV format.\n\n        Creates a CSV file with one row per building and one column per year.\n        Each cell contains the upgrade ID for that building in that year.\n\n        Args:\n            output_path: Path where the CSV file should be written (local or S3).\n\n        Output format:\n            bldg_id,year_0,year_1,year_2\n            405821,0,0,0\n            612547,0,4,4\n            789234,0,0,8\n\n        Raises:\n            ScenarioDataNotFoundError: If data for scenario upgrades is not on disk.\n\n        Example:\n            &gt;&gt;&gt; mus.export_scenario_to_cairo(\"./scenario.csv\")\n            Exported scenario for 1000 buildings across 3 years to ./scenario.csv\n        \"\"\"\n        self._validate_data_availability(\"metadata\")\n        bldgs, allocations = self._allocation_plan\n        sorted_bldg_ids = sorted(self.sampled_bldgs)\n        # Index once for stable, low-overhead column fills.\n        idx = {bid: i for i, bid in enumerate(sorted_bldg_ids)}\n        total = len(bldgs)\n\n        year_cols = {f\"year_{year_idx}\": [0] * len(sorted_bldg_ids) for year_idx in range(self.num_years)}\n        for year_idx in range(self.num_years):\n            col = year_cols[f\"year_{year_idx}\"]\n            # Fill each year column from the precomputed adopter slices.\n            for uid in self._upgrade_ids:\n                target = int(self.scenario[uid][year_idx] * total)\n                if target:\n                    for bldg_id in allocations[uid][:target]:\n                        col[idx[bldg_id]] = uid\n\n        df = pl.DataFrame({\"bldg_id\": sorted_bldg_ids, **year_cols})\n        resolved = _resolve_path(output_path)\n        df.write_csv(str(resolved))\n\n        print(f\"Exported scenario for {len(self.sampled_bldgs)} buildings across {self.num_years} years to {resolved}\")\n\n    def _resolve_scenario_root(self, path: Path | S3Path | str | None) -&gt; Path | S3Path:\n        \"\"\"Resolve scenario root directory based on optional base path.\"\"\"\n        base_path = self.data_path / self.release.key if path is None else _resolve_path(path)\n        return base_path / \"mixed_upgrade\" / self.scenario_name\n\n    def save_metadata_parquet(self, path: Path | S3Path | str | None = None) -&gt; None:\n        \"\"\"Save mixed upgrade metadata to a partitioned parquet dataset.\n\n        Output structure:\n            &lt;path&gt;/&lt;scenario_name&gt;/year=&lt;year_int&gt;/metadata.parquet\n\n        Args:\n            path: Optional base path to write to. Defaults to data_path/release/mixed_upgrade.\n        \"\"\"\n        scenario_root = self._resolve_scenario_root(path)\n        for year_idx in range(self.num_years):\n            year_dir = scenario_root / f\"year={year_idx}\"\n            if isinstance(year_dir, Path):\n                year_dir.mkdir(parents=True, exist_ok=True)\n            output_file = year_dir / \"metadata.parquet\"\n            self.read_metadata(years=[year_idx]).sink_parquet(str(output_file))\n\n    def save_hourly_load_parquet(self, path: Path | S3Path | str | None = None) -&gt; None:\n        \"\"\"Save mixed upgrade hourly load curves to partitioned parquet datasets.\n\n        Output structure:\n            &lt;path&gt;/mixed_upgrade/&lt;scenario_name&gt;/year=&lt;year_int&gt;/&lt;bldg_id&gt;-&lt;upgrade_id&gt;.parquet\n\n        Args:\n            path: Optional base path to write to. Defaults to the release path.\n        \"\"\"\n        scenario_root = self._resolve_scenario_root(path)\n        for year_idx in range(self.num_years):\n            year_dir = scenario_root / f\"year={year_idx}\"\n            if isinstance(year_dir, Path):\n                year_dir.mkdir(parents=True, exist_ok=True)\n            df = self.read_load_curve_hourly(years=[year_idx]).collect()\n            if df.is_empty():\n                continue\n            for key, group in df.partition_by([\"bldg_id\", \"upgrade_id\"], as_dict=True).items():\n                bldg_id, upgrade_id = key\n                output_file = year_dir / f\"{int(bldg_id)}-{int(upgrade_id)}.parquet\"\n                group.write_parquet(str(output_file))\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.downloaded_metadata","title":"<code>downloaded_metadata</code>  <code>cached</code> <code>property</code>","text":"<p>Cached property for metadata file discovery only.</p>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.materialized_scenario","title":"<code>materialized_scenario</code>  <code>cached</code> <code>property</code>","text":"<p>Materialize building allocations for all years in the scenario.</p>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.export_scenario_to_cairo","title":"<code>export_scenario_to_cairo(output_path)</code>","text":"<p>Export scenario to CAIRO-compatible CSV format.</p> <p>Creates a CSV file with one row per building and one column per year. Each cell contains the upgrade ID for that building in that year.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | Path | S3Path</code> <p>Path where the CSV file should be written (local or S3).</p> required Output format <p>bldg_id,year_0,year_1,year_2 405821,0,0,0 612547,0,4,4 789234,0,0,8</p> <p>Raises:</p> Type Description <code>ScenarioDataNotFoundError</code> <p>If data for scenario upgrades is not on disk.</p> Example <p>mus.export_scenario_to_cairo(\"./scenario.csv\") Exported scenario for 1000 buildings across 3 years to ./scenario.csv</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def export_scenario_to_cairo(self, output_path: str | Path | S3Path) -&gt; None:\n    \"\"\"Export scenario to CAIRO-compatible CSV format.\n\n    Creates a CSV file with one row per building and one column per year.\n    Each cell contains the upgrade ID for that building in that year.\n\n    Args:\n        output_path: Path where the CSV file should be written (local or S3).\n\n    Output format:\n        bldg_id,year_0,year_1,year_2\n        405821,0,0,0\n        612547,0,4,4\n        789234,0,0,8\n\n    Raises:\n        ScenarioDataNotFoundError: If data for scenario upgrades is not on disk.\n\n    Example:\n        &gt;&gt;&gt; mus.export_scenario_to_cairo(\"./scenario.csv\")\n        Exported scenario for 1000 buildings across 3 years to ./scenario.csv\n    \"\"\"\n    self._validate_data_availability(\"metadata\")\n    bldgs, allocations = self._allocation_plan\n    sorted_bldg_ids = sorted(self.sampled_bldgs)\n    # Index once for stable, low-overhead column fills.\n    idx = {bid: i for i, bid in enumerate(sorted_bldg_ids)}\n    total = len(bldgs)\n\n    year_cols = {f\"year_{year_idx}\": [0] * len(sorted_bldg_ids) for year_idx in range(self.num_years)}\n    for year_idx in range(self.num_years):\n        col = year_cols[f\"year_{year_idx}\"]\n        # Fill each year column from the precomputed adopter slices.\n        for uid in self._upgrade_ids:\n            target = int(self.scenario[uid][year_idx] * total)\n            if target:\n                for bldg_id in allocations[uid][:target]:\n                    col[idx[bldg_id]] = uid\n\n    df = pl.DataFrame({\"bldg_id\": sorted_bldg_ids, **year_cols})\n    resolved = _resolve_path(output_path)\n    df.write_csv(str(resolved))\n\n    print(f\"Exported scenario for {len(self.sampled_bldgs)} buildings across {self.num_years} years to {resolved}\")\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_load_curve_15min","title":"<code>read_load_curve_15min(years=None)</code>","text":"<p>Read 15-minute load curve data for specified years in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int] | None</code> <p>List of year indices to include, or None for all years.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with columns: - bldg_id: Building ID - upgrade_id: Upgrade ID - year: Year index - timestamp: Timestamp of the load data - ...: Energy columns (e.g., out.electricity.total.energy_consumption)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any year index is out of range.</p> <code>ScenarioDataNotFoundError</code> <p>If load curve data for scenario upgrades is not on disk.</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def read_load_curve_15min(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n    \"\"\"Read 15-minute load curve data for specified years in the scenario.\n\n    Args:\n        years: List of year indices to include, or None for all years.\n\n    Returns:\n        A Polars LazyFrame with columns:\n            - bldg_id: Building ID\n            - upgrade_id: Upgrade ID\n            - year: Year index\n            - timestamp: Timestamp of the load data\n            - ...: Energy columns (e.g., out.electricity.total.energy_consumption)\n\n    Raises:\n        ValueError: If any year index is out of range.\n        ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n    \"\"\"\n    return self._read_data_for_scenario(\"load_curve_15min\", years)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_load_curve_annual","title":"<code>read_load_curve_annual(years=None)</code>","text":"<p>Read annual load curve data for specified years in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int] | None</code> <p>List of year indices to include, or None for all years.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with columns: - bldg_id: Building ID - upgrade_id: Upgrade ID - year: Year index - ...: Annual energy totals</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any year index is out of range.</p> <code>ScenarioDataNotFoundError</code> <p>If load curve data for scenario upgrades is not on disk.</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def read_load_curve_annual(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n    \"\"\"Read annual load curve data for specified years in the scenario.\n\n    Args:\n        years: List of year indices to include, or None for all years.\n\n    Returns:\n        A Polars LazyFrame with columns:\n            - bldg_id: Building ID\n            - upgrade_id: Upgrade ID\n            - year: Year index\n            - ...: Annual energy totals\n\n    Raises:\n        ValueError: If any year index is out of range.\n        ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n    \"\"\"\n    return self._read_data_for_scenario(\"load_curve_annual\", years)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_load_curve_daily","title":"<code>read_load_curve_daily(years=None)</code>","text":"<p>Read daily load curve data for specified years in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int] | None</code> <p>List of year indices to include, or None for all years.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with columns: - bldg_id: Building ID - upgrade_id: Upgrade ID - year: Year index - timestamp: Timestamp of the load data - ...: Energy columns</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any year index is out of range.</p> <code>ScenarioDataNotFoundError</code> <p>If load curve data for scenario upgrades is not on disk.</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def read_load_curve_daily(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n    \"\"\"Read daily load curve data for specified years in the scenario.\n\n    Args:\n        years: List of year indices to include, or None for all years.\n\n    Returns:\n        A Polars LazyFrame with columns:\n            - bldg_id: Building ID\n            - upgrade_id: Upgrade ID\n            - year: Year index\n            - timestamp: Timestamp of the load data\n            - ...: Energy columns\n\n    Raises:\n        ValueError: If any year index is out of range.\n        ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n    \"\"\"\n    return self._read_data_for_scenario(\"load_curve_daily\", years)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_load_curve_hourly","title":"<code>read_load_curve_hourly(years=None)</code>","text":"<p>Read hourly load curve data for specified years in the scenario.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int] | None</code> <p>List of year indices to include, or None for all years.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with columns: - bldg_id: Building ID - upgrade_id: Upgrade ID - year: Year index - timestamp: Timestamp of the load data - ...: Energy columns</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any year index is out of range.</p> <code>ScenarioDataNotFoundError</code> <p>If load curve data for scenario upgrades is not on disk.</p> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def read_load_curve_hourly(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n    \"\"\"Read hourly load curve data for specified years in the scenario.\n\n    Args:\n        years: List of year indices to include, or None for all years.\n\n    Returns:\n        A Polars LazyFrame with columns:\n            - bldg_id: Building ID\n            - upgrade_id: Upgrade ID\n            - year: Year index\n            - timestamp: Timestamp of the load data\n            - ...: Energy columns\n\n    Raises:\n        ValueError: If any year index is out of range.\n        ScenarioDataNotFoundError: If load curve data for scenario upgrades is not on disk.\n    \"\"\"\n    return self._read_data_for_scenario(\"load_curve_hourly\", years)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_metadata","title":"<code>read_metadata(years=None)</code>","text":"<p>Read metadata for specified years in the scenario.</p> <p>Returns a LazyFrame containing metadata for all buildings and years in the scenario. Each row represents one building in one year.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int] | None</code> <p>List of year indices to include (0-indexed), or None for all years. Example: [0, 1, 2] or None</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with columns: - bldg_id: Building ID (from sampled baseline) - upgrade_id: Upgrade ID for this building in this year (0 or scenario upgrade) - year: Year index (0-indexed) - ...: Original metadata columns (e.g., in.state, in.vintage, etc.)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any year index is out of range.</p> <code>ScenarioDataNotFoundError</code> <p>If metadata for scenario upgrades is not on disk.</p> Example Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def read_metadata(self, years: list[int] | None = None) -&gt; pl.LazyFrame:\n    \"\"\"Read metadata for specified years in the scenario.\n\n    Returns a LazyFrame containing metadata for all buildings and years in the\n    scenario. Each row represents one building in one year.\n\n    Args:\n        years: List of year indices to include (0-indexed), or None for all years.\n            Example: [0, 1, 2] or None\n\n    Returns:\n        A Polars LazyFrame with columns:\n            - bldg_id: Building ID (from sampled baseline)\n            - upgrade_id: Upgrade ID for this building in this year (0 or scenario upgrade)\n            - year: Year index (0-indexed)\n            - ...: Original metadata columns (e.g., in.state, in.vintage, etc.)\n\n    Raises:\n        ValueError: If any year index is out of range.\n        ScenarioDataNotFoundError: If metadata for scenario upgrades is not on disk.\n\n    Example:\n        &gt;&gt;&gt; # Read metadata for all years\n        &gt;&gt;&gt; metadata = mus.read_metadata()\n        &gt;&gt;&gt; df = metadata.collect()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Read metadata for specific years\n        &gt;&gt;&gt; metadata_early = mus.read_metadata(years=[0, 1])\n    \"\"\"\n    return self._read_data_for_scenario(\"metadata\", years)\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_metadata--read-metadata-for-all-years","title":"Read metadata for all years","text":"<p>metadata = mus.read_metadata() df = metadata.collect()</p>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.read_metadata--read-metadata-for-specific-years","title":"Read metadata for specific years","text":"<p>metadata_early = mus.read_metadata(years=[0, 1])</p>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.save_hourly_load_parquet","title":"<code>save_hourly_load_parquet(path=None)</code>","text":"<p>Save mixed upgrade hourly load curves to partitioned parquet datasets.</p> Output structure <p>/mixed_upgrade//year=/-.parquet <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | S3Path | str | None</code> <p>Optional base path to write to. Defaults to the release path.</p> <code>None</code> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def save_hourly_load_parquet(self, path: Path | S3Path | str | None = None) -&gt; None:\n    \"\"\"Save mixed upgrade hourly load curves to partitioned parquet datasets.\n\n    Output structure:\n        &lt;path&gt;/mixed_upgrade/&lt;scenario_name&gt;/year=&lt;year_int&gt;/&lt;bldg_id&gt;-&lt;upgrade_id&gt;.parquet\n\n    Args:\n        path: Optional base path to write to. Defaults to the release path.\n    \"\"\"\n    scenario_root = self._resolve_scenario_root(path)\n    for year_idx in range(self.num_years):\n        year_dir = scenario_root / f\"year={year_idx}\"\n        if isinstance(year_dir, Path):\n            year_dir.mkdir(parents=True, exist_ok=True)\n        df = self.read_load_curve_hourly(years=[year_idx]).collect()\n        if df.is_empty():\n            continue\n        for key, group in df.partition_by([\"bldg_id\", \"upgrade_id\"], as_dict=True).items():\n            bldg_id, upgrade_id = key\n            output_file = year_dir / f\"{int(bldg_id)}-{int(upgrade_id)}.parquet\"\n            group.write_parquet(str(output_file))\n</code></pre>"},{"location":"modules/#buildstock_fetch.mixed_upgrade.MixedUpgradeScenario.save_metadata_parquet","title":"<code>save_metadata_parquet(path=None)</code>","text":"<p>Save mixed upgrade metadata to a partitioned parquet dataset.</p> Output structure <p>//year=/metadata.parquet <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | S3Path | str | None</code> <p>Optional base path to write to. Defaults to data_path/release/mixed_upgrade.</p> <code>None</code> Source code in <code>buildstock_fetch/mixed_upgrade.py</code> <pre><code>def save_metadata_parquet(self, path: Path | S3Path | str | None = None) -&gt; None:\n    \"\"\"Save mixed upgrade metadata to a partitioned parquet dataset.\n\n    Output structure:\n        &lt;path&gt;/&lt;scenario_name&gt;/year=&lt;year_int&gt;/metadata.parquet\n\n    Args:\n        path: Optional base path to write to. Defaults to data_path/release/mixed_upgrade.\n    \"\"\"\n    scenario_root = self._resolve_scenario_root(path)\n    for year_idx in range(self.num_years):\n        year_dir = scenario_root / f\"year={year_idx}\"\n        if isinstance(year_dir, Path):\n            year_dir.mkdir(parents=True, exist_ok=True)\n        output_file = year_dir / \"metadata.parquet\"\n        self.read_metadata(years=[year_idx]).sink_parquet(str(output_file))\n</code></pre>"},{"location":"modules/#buildstock_fetch.scenarios.uniform_adoption","title":"<code>buildstock_fetch.scenarios.uniform_adoption(upgrade_ids, weights, adoption_trajectory)</code>","text":"<p>Generate a scenario from total adoption trajectory and fixed upgrade weights.</p> <p>This helper function distributes a total adoption trajectory across multiple upgrades according to fixed weights. For example, if 30% of buildings adopt in year 1, and upgrade 4 has weight 0.6, then 18% of buildings will adopt upgrade 4 in year 1.</p> <p>Parameters:</p> Name Type Description Default <code>upgrade_ids</code> <code>list[int]</code> <p>List of upgrade IDs to include in the scenario.</p> required <code>weights</code> <code>dict[int, float]</code> <p>Per-upgrade share of total adopters. Must sum to 1.0 (\u00b11e-6). Example: {4: 0.6, 8: 0.4} means 60% choose upgrade 4, 40% choose upgrade 8.</p> required <code>adoption_trajectory</code> <code>list[float]</code> <p>Total adoption fraction per year. Must be non-decreasing. Example: [0.1, 0.3, 0.5] means 10%, 30%, 50% total adoption over 3 years.</p> required <p>Returns:</p> Type Description <code>dict[int, list[float]]</code> <p>Scenario dict mapping upgrade IDs to per-year adoption fractions.</p> <p>Raises:</p> Type Description <code>InvalidScenarioError</code> <p>If weights don't sum to 1.0, contain invalid values, or if adoption_trajectory is invalid.</p> Example <p>scenario = uniform_adoption( ...     upgrade_ids=[4, 8], ...     weights={4: 0.6, 8: 0.4}, ...     adoption_trajectory=[0.1, 0.3, 0.5], ... ) scenario {4: [0.06, 0.18, 0.30], 8: [0.04, 0.12, 0.20]}</p> Source code in <code>buildstock_fetch/scenarios.py</code> <pre><code>def uniform_adoption(\n    upgrade_ids: list[int],\n    weights: dict[int, float],\n    adoption_trajectory: list[float],\n) -&gt; dict[int, list[float]]:\n    \"\"\"Generate a scenario from total adoption trajectory and fixed upgrade weights.\n\n    This helper function distributes a total adoption trajectory across multiple\n    upgrades according to fixed weights. For example, if 30% of buildings adopt\n    in year 1, and upgrade 4 has weight 0.6, then 18% of buildings will adopt\n    upgrade 4 in year 1.\n\n    Args:\n        upgrade_ids: List of upgrade IDs to include in the scenario.\n        weights: Per-upgrade share of total adopters. Must sum to 1.0 (\u00b11e-6).\n            Example: {4: 0.6, 8: 0.4} means 60% choose upgrade 4, 40% choose upgrade 8.\n        adoption_trajectory: Total adoption fraction per year. Must be non-decreasing.\n            Example: [0.1, 0.3, 0.5] means 10%, 30%, 50% total adoption over 3 years.\n\n    Returns:\n        Scenario dict mapping upgrade IDs to per-year adoption fractions.\n\n    Raises:\n        InvalidScenarioError: If weights don't sum to 1.0, contain invalid values,\n            or if adoption_trajectory is invalid.\n\n    Example:\n        &gt;&gt;&gt; scenario = uniform_adoption(\n        ...     upgrade_ids=[4, 8],\n        ...     weights={4: 0.6, 8: 0.4},\n        ...     adoption_trajectory=[0.1, 0.3, 0.5],\n        ... )\n        &gt;&gt;&gt; scenario\n        {4: [0.06, 0.18, 0.30], 8: [0.04, 0.12, 0.20]}\n    \"\"\"\n    # Validate all parameters\n    _validate_upgrade_ids_param(upgrade_ids)\n    _validate_weights_param(weights, upgrade_ids)\n    _validate_adoption_trajectory(adoption_trajectory)\n\n    # Generate scenario by multiplying trajectory by weights\n    scenario: dict[int, list[float]] = {}\n    for upgrade_id in upgrade_ids:\n        weight = weights[upgrade_id]\n        scenario[upgrade_id] = [weight * adoption for adoption in adoption_trajectory]\n\n    # Validate the generated scenario\n    validate_scenario(scenario)\n\n    return scenario\n</code></pre>"},{"location":"usage/","title":"BuildStock Fetch","text":"<p>A Python CLI tool for downloading building energy simulation data from the National Renewable Energy Laboratory (NREL) ResStock and ComStock projects.</p>"},{"location":"usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Background</li> <li>Key Features</li> <li>Data source</li> <li>Available datasets</li> <li>Installation</li> <li>Quick Start</li> <li>Usage Examples</li> <li>Command Line Interface</li> </ul>"},{"location":"usage/#background","title":"Background","text":"<p>NREL's ResStock and ComStock projects, collectively known as BuildStock, offer a statistically representative snapshot of buildings across the United States. These realistic-but-not-real buildings are then loaded into building energy modeling (BEM) software EnergyPlus, in order to estimate their weather-dependent energy consumption and water use by simulating heating, cooling, lighting, ventilation, and other energy flows.</p> <p>The simulations output end-use load profiles, which NREL releases as open data. But it's also possible to do your own Buildstock simulations using the same building and weather file that NREL uses.</p> <p>BuildStock Fetch is a python library that makes it easy to download NREL's end-use load profiles, as well as the hpxml, weather, and other input files needed to run your simulations using OCHRE a lightweight BEM library also developed by NREL.</p>"},{"location":"usage/#key-features","title":"Key Features","text":"<ul> <li>Comprehensive Data Access: Download physical building characteristics, load and occupancy schedules, simulated load curves, and weather files.</li> <li>Flexible Selection: Choose specific states, upgrade scenarios, and building types.</li> <li>Organized Output: Structured file organization by release version, state, and building type.</li> </ul>"},{"location":"usage/#data-source","title":"Data source","text":"<p>BuildStock Fetch (<code>bsf</code>) makes it easier to download the files available on NREL's public aws s3 buckets.</p> <p>Besides weather files, all the data that <code>bsf</code> allows you to download is on building units, either residential dwellings or commercial establishments.</p>"},{"location":"usage/#buildstock-releases","title":"Buildstock Releases","text":"<p>These units are grouped into Buildstock releases, which are defined by:</p> <ul> <li> <p>Product: either ResStock (for residential buildings) or ComStock (for commercial buildings).</p> </li> <li> <p>Release Year: The year of release. NREL's first release was in 2021, and each subsequent year has seen its own set of new releases.</p> </li> <li> <p>Weather File: Building energy simulations require weather files, which represent temperature, moisture, solar radiation, and other environmental condition. NREL has usef three different weather files for ComStock and ResStock: <code>tmy3</code>, <code>amy2012</code>, and <code>amy2018</code>.</p> </li> <li><code>tmy3</code> stands for Typical Meteorological Year 3. This is a synthetic weather dataset that represent a \"typical\" annual weather conditions and were built using historical weather data from 1991-2005.</li> <li><code>amy2012</code> stands for Actual Meteorological Year 2012, and represents actual weather data from 2012.</li> <li> <p><code>amy2018</code> stands for Actual Meteorological Year 2018, and represents actual weather data from 2018.</p> </li> <li> <p>Release Version: Most releases have only one version, which is version 1. NREL will occasionally correct or update a release, these releases will have versions 1.1 or 2.</p> </li> <li> <p>Upgrades: that main value of ResStock and ComStock is the ability to run \"what if?\" scenarios, where building shells and equipment are altered in order to compare the resulting simulated load curve to the current baseline building stock. Each release has its own distinct list of numbered upgrade scenarios, representing various building envelope and equipment upgrades, including weatherization packages, efficient electrical appliances, and so on.</p> </li> </ul>"},{"location":"usage/#available-datasets","title":"Available datasets","text":"<p><code>bsf</code> makes it easier to download three kinds of files:</p> <ul> <li> <p>Building Unit Metadata: The metadata table offers information each building unit for each upgrade within each release. This information includes the building unit's <code>bldg_id</code>, overall physical characteristics (square footage, number of rooms, etc.), the building envelope, all energy consuming devices, air infilation levels, approximate physical location, income range, and so on. These building units are synthetic, drawn from a stastistical distribution of the US building stock assembled from dozens of surveys and other datasets.</p> </li> <li> <p>End Use Load Profiles: NREL maps these synthetic buildings to geometric models in EnergyPlus, and runs physics simulations with particular weather files in order to generate energy consumption load curves for every appliances in every building unit in a given release. NREL then publishes these simulation outputs under the End Use Load Profiles data product. These datasets provide detailed simulation input parameters and load outputs, such as fuel consumption by usage type (e.g. space heating/cooling, water heating, lights, etc.), and building conditions (e.g. indoor temperature and humidity, hot water temperature, etc.). NREL publishes these profiles at 15-min time granularity for each building unit in a particular release, as well as yearly aggregates. Load are often needed at the hourly or monthly level for anaysis, so <code>bsf</code> plans to offer these pre-aggregated datasets in the future to save you time.</p> </li> <li> <p>Building Simulation Input Data: NREL also makes available the input files they use to run these simulations, which are the building metadata in HPXML format, equipment and occupancy schedule files, and weather files. You can use these input files to run your own simulations of Buildstock building units, either in EnergyPlus (via buildstock-batch) or in OCHRE, a new building energy simulation tool developed by NREL. OCHRE is meant to be lighter-weight alternative to other numerical simulation softwares such as EnergyPlus or TRNSYS. OCHRE requires as input a building HPXML file that provides information on the building envelope and HVAC and water heating equipment, the schedule files that provide load profiles for non-controllable appliance use such as lights or dishwasher and occupancy levels, and weather files. Each building HPXML file has a corresponding weather station ID based on its location, and <code>bsf</code> can fetch this for you too.</p> </li> </ul>"},{"location":"usage/#installation","title":"Installation","text":""},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li><code>pip</code> or <code>uv</code> package manager</li> </ul>"},{"location":"usage/#install-from-source","title":"Install from Source","text":"<p>BuildStock Fetch is not yet available as a published library on PyPI. In the meantime, the library can be installed directly from the GitHub repository using:</p> <pre><code>pip3 install git+https://github.com/switchbox-data/buildstock-fetch.git@main\n</code></pre>"},{"location":"usage/#quick-start","title":"Quick Start","text":"<p>To download data with BuildStock Fetch, you must specify the following variables: product type, release year, weather file type, release version, upgrade scenarios, state, types of files to download, and output directory. Once these inputs have been specified, BuildStock Fetch will first resolved the individual <code>bldg_ids</code> correspond to this  selection, and then download the requested files pertaining to each building unit to the specified output directory.</p>"},{"location":"usage/#interactive-mode","title":"Interactive Mode","text":"<p>The easiest way to get started is using the interactive CLI:</p> <pre><code>bsf\n</code></pre> <p>This will guide you through:</p> <ol> <li>Selecting product type (ResStock/ComStock)</li> <li>Choosing release year</li> <li>Choosing the weather file type</li> <li>Choosing the release version</li> <li>Selecting the upgrade scenarios (you can choose multiple)</li> <li>Selecting the state (you can choose multiple)</li> <li>Selecting the file types to download (you can choose multiple)</li> <li>Specifying the output directory for the downloaded files.</li> </ol> <p>The interactive CLI mode only shows valid release options and file types, based on what's currently available on NREL's s3 bucket.</p>"},{"location":"usage/#downloading-a-sample","title":"Downloading a sample","text":"<p>Once all the inputs have been collected, BuildStock Fetch will ask whether to download all files for the release, or just a sample. Each release has around 10,000 to 30,000 building units associated with it. If the user wishes to download just a small sample, they can specify the number of files to download for each state-upgrade version pair.</p> <p>You can also specify the sample size via <code>--sample</code> command-line argument. Use <code>0</code> to download all files.</p>"},{"location":"usage/#direct-command-line","title":"Direct Command Line","text":"<p>You can also use specify the buildstock release, file types, and geography in one go, using direct command line arguments:</p> <pre><code>bsf \\\n  --product resstock \\\n  --release_year 2022 \\\n  --weather_file tmy3 \\\n  --release_version 1 \\\n  --states \"CA NY\" \\\n  --file_type \"metadata load_curve_15min\" \\\n  --upgrade_id \"0 1\" \\\n  --output_directory ./data \\\n  --sample 0\n</code></pre> <p><code>bsf</code> will warn you if you've specified an invalid buildstock release, state, file type, upgrade, or output directory.</p> <p>You can provide only some of the arguments, in which case the app will launch in interactive mode, and ask for parameters you did not provide yet.</p>"},{"location":"usage/#specifying-the-number-of-concurrent-downloads","title":"Specifying the number of concurrent downloads","text":"<p>By default, <code>bsf</code> will limit the number of concurrent downloads to 15. You may increase or decrease that number by providing <code>--tasks &lt;number&gt;</code> command-line argument (i.e. <code>--tasks: 5</code> will limit concurrent downloads to 5 files at a time)</p>"},{"location":"usage/#output-structure","title":"Output Structure","text":"<p>The <code>--output_directory</code> option specifies the top directory where the files will be downloaded. From there, the files will be organized as:</p> <pre><code>output_directory/\n\u2514\u2500\u2500 release_name/\n    \u2514\u2500\u2500 file_type/\n        \u2514\u2500\u2500 state/\n            \u2514\u2500\u2500 bldgid1_file\n            \u2514\u2500\u2500 bldgid2_file\n            \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"usage/#usage-examples","title":"Usage Examples","text":""},{"location":"usage/#example-1-download-metadata-for-california","title":"Example 1: Download Metadata for California","text":"<pre><code>bsf \\\n  --product resstock \\\n  --release_year 2022 \\\n  --weather_file tmy3 \\\n  --release_version 1 \\\n  --states CA \\\n  --file_type metadata \\\n  --upgrade_id 0 \\\n  --output_directory ./california_data\n</code></pre>"},{"location":"usage/#example-2-download-sample-building-files","title":"Example 2: Download Sample Building Files","text":"<pre><code>bsf \\\n  --product comstock \\\n  --release_year 2024 \\\n  --weather_file amy2018 \\\n  --release_version 1 \\\n  --states \"CA TX NY\" \\\n  --file_type \"hpxml schedule\" \\\n  --upgrade_id \"0 1 2\" \\\n  --output_directory ./sample_buildings\n</code></pre> <p>Output Structure:</p> <pre><code>sample_buildings/\n\u2514\u2500\u2500 com_2024_amy2018_1/\n    \u251c\u2500\u2500 hpxml/\n    \u2502   \u251c\u2500\u2500 CA/\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_0.xml\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_1.xml\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_2.xml\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id2_up_0.xml\n    \u2502   \u2502   \u251c\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 TX/\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_0.xml\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_1.xml\n    \u2502   \u2502   \u251c\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 NY/\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_0.xml\n    \u2502   \u2502   \u251c\u2500\u2500 bldg_id1_up_1.xml\n    \u2502   \u2502   \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 schedule/\n        \u251c\u2500\u2500 CA/\n        \u2502   \u251c\u2500\u2500 bldg_id1_up_0_schedule.csv\n        \u2502   \u251c\u2500\u2500 bldg_id1_up_1_schedule.csv\n        \u2502   \u251c\u2500\u2500 bldg_id1_up_2_schedule.csv\n        \u2502   \u251c\u2500\u2500 bldg_id2_up_0_schedule.csv\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"usage/#example-3-download-load-curves-for-analysis","title":"Example 3: Download Load Curves for Analysis","text":"<pre><code>bsf \\\n  --product resstock \\\n  --release_year 2022 \\\n  --weather_file tmy3 \\\n  --release_version 1 \\\n  --states \"CA MA NY\" \\\n  --file_type \"load_curve_15min\" \\\n  --upgrade_id \"0 1\" \\\n  --output_directory ./load_analysis\n</code></pre>"},{"location":"usage/#command-line-interface","title":"Command Line Interface","text":""},{"location":"usage/#interactive-mode_1","title":"Interactive Mode","text":"<p>When run without arguments, BuildStock Fetch launches an interactive interface:</p> <pre><code>BuildStock Fetch Interactive CLI\nWelcome to the BuildStock Fetch CLI!\nThis tool allows you to fetch data from the NREL BuildStock API.\nPlease select the release information and file type you would like to fetch:\n\nSelect product type: [resstock, comstock]\nSelect release year: [2021, 2022, 2023, 2024, ...]\nSelect weather file: [tmy3, amy2012, amy2018]\nSelect release version: [1, 1.1, 2]\nSelect upgrade ids: [0, 1, 2, 3, 4, 5, 6, 7, ...]\nSelect states: [AL, AZ, AR, CA, ...]\nSelect file type: [hpxml, schedule, metadata, load_curve_15min, ...]\nSelect output directory: ./data\n</code></pre>"},{"location":"usage/#command-line-options","title":"Command Line Options","text":"Option Short Description Example <code>--product</code> <code>-p</code> Product type (resstock/comstock) <code>--product resstock</code> <code>--release_year</code> <code>-y</code> Release year <code>--release_year 2022</code> <code>--weather_file</code> <code>-w</code> Weather file type <code>--weather_file tmy3</code> <code>--release_version</code> <code>-r</code> Release version <code>--release_version 1</code> <code>--states</code> <code>-s</code> States (space-separated) <code>--states \"CA NY TX\"</code> <code>--file_type</code> <code>-f</code> File types (space-separated) <code>--file_type \"metadata hpxml\"</code> <code>--upgrade_id</code> <code>-u</code> Upgrade IDs (space-separated) <code>--upgrade_id \"0 1 2\"</code> <code>--output_directory</code> <code>-o</code> Output directory <code>--output_directory ./data</code> <p>You can also run</p> <pre><code>bsf --help\n</code></pre> <p>to see these options</p>"},{"location":"usage/#available-file-types","title":"Available File Types","text":"<ul> <li><code>hpxml</code>: Building energy model files in HPXML format</li> <li><code>schedule</code>: Occupancy and equipment schedules</li> <li><code>metadata</code>: Building characteristics and metadata</li> <li><code>load_curve_15min</code>: 15-minute resolution load profiles</li> <li><code>load_curve_hourly</code>: Hourly resolution load profiles</li> <li><code>load_curve_daily</code>: Daily resolution load profiles</li> <li><code>load_curve_monthly</code>: Monthly resolution load profiles</li> <li><code>load_curve_annual</code>: Annual resolution load profiles</li> </ul>"},{"location":"usage/#python-api","title":"Python API","text":"<p>You can also use BuildStock Fetch programmatically:</p> <pre><code>from buildstock_fetch.main import fetch_bldg_ids, fetch_bldg_data\nfrom pathlib import Path\n\n# Get building IDs for a specific state\nbldg_ids = fetch_bldg_ids(\n    product=\"resstock\",\n    release_year=\"2022\",\n    weather_file=\"tmy3\",\n    release_version=\"1\",\n    state=\"CA\",\n    upgrade_id=\"0\"\n)\n\n# Download data\ndownloaded_paths, failed_downloads = fetch_bldg_data(\n    bldg_ids=bldg_ids,\n    file_type=(\"metadata\", \"load_curve_15min\"),\n    output_dir=Path(\"./data\")\n)\n</code></pre>"}]}